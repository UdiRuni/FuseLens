{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyenaDNA Binary Classifier for DNA Breakpoint Detection\n",
    "\n",
    "This notebook implements a complete pipeline to fine-tune HyenaDNA for classifying DNA sequences as positive or negative breakpoints, similar to FusionAI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**What this notebook does:**\n",
    "1. **Prepares data** from FusionAI-format CSV files (positive/negative breakpoints)\n",
    "2. **Merges sequences** (5' + 3' = 20kb input)\n",
    "3. **Fine-tunes HyenaDNA** with a binary classification head\n",
    "4. **Makes predictions** on test data\n",
    "\n",
    "**Key Features:**\n",
    "- Handles sequences up to 1M bp (we'll use 20kb)\n",
    "- Uses Hugging Face Trainer for optimization\n",
    "- Automatic mixed precision (FP16) for faster training\n",
    "- Comprehensive metrics (Accuracy, Precision, Recall, F1, AUC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup\n",
    "\n",
    "First, let's install all required dependencies and check our compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# !pip install -q torch transformers accelerate pandas scikit-learn\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU Device: NVIDIA A100 80GB PCIe\n",
      "GPU Memory: 84.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akp1/GeneAI/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and setup\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Consider using a GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akp1/GeneAI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "from safetensors.torch import load_model, save_model\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Set all hyperparameters and file paths here. **Update the CSV paths to match your data!**\n",
    "\n",
    "### Model Selection Guide:\n",
    "- `hyenadna-tiny-1k-seqlen` - Max 1kb (‚ùå Too short for 20kb!)\n",
    "- `hyenadna-small-32k-seqlen` - Max 32kb (‚úÖ **Recommended for 20kb**)\n",
    "- `hyenadna-medium-160k-seqlen` - Max 160kb (Better performance, more memory)\n",
    "- `hyenadna-large-1m-seqlen` - Max 1M (Best performance, requires A100 80GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model: LongSafari/hyenadna-medium-160k-seqlen-hf\n",
      "Batch size: 8\n",
      "Training epochs: 3\n"
     ]
    }
   ],
   "source": [
    "# ============ FILE PATHS - UPDATE THESE! ============\n",
    "POSITIVE_CSV = \"datasets/fusion_gene_positive_bp_information_with_class_for_modeling.txt\"  # Path to positive breakpoint CSV\n",
    "NEGATIVE_CSV = \"datasets/fusion_gene_negative_bp_information_with_class_for_modeling.txt\"  # Path to negative breakpoint CSV\n",
    "TEST_CSV = \"datasets/fusion_gene_positive_bp_information_with_class_for_testing.txt\"          # Path to test data CSV\n",
    "COLUMNS = [\"Hgene\",\"Hchr\",\"Hbp\",\"Hstrand\",\"Tgene\",\"Tchr\",\"Tbp\",\"Tstrand\",\"5'-gene sequence (10Kb)\",\"3'-gene sequence (10Kb)\"] # CSV Columns names\n",
    "OUTPUT_DIR = \"./hyenadna_breakpoint_model_160K\" # Where to save the model\n",
    "\n",
    "# ============ MODEL CONFIGURATION ============\n",
    "MODEL_NAME = \"LongSafari/hyenadna-medium-160k-seqlen-hf\"  # Recommended for 20kb sequences\n",
    "MAX_LENGTH = 20480  # Maximum sequence length (20kb)\n",
    "\n",
    "# ============ TRAINING HYPERPARAMETERS ============\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 8          # Adjust based on GPU memory (2 for T4, 4 for V100, 8 for A100)\n",
    "LEARNING_RATE = 2e-5    # Learning rate\n",
    "WARMUP_STEPS = 1000      # Learning rate warmup\n",
    "WEIGHT_DECAY = 0.01     # L2 regularization\n",
    "VAL_SPLIT = 0.2         # Validation split (20%)\n",
    "\n",
    "# ============ DEVICE SETUP ============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Preparation Class\n",
    "\n",
    "This class handles loading and preparing the FusionAI CSV files.\n",
    "\n",
    "**Input CSV Format (tab-separated):**\n",
    "- `Hgene`, `Hchr`, `Hbp`, `Hstrand` - Head gene information\n",
    "- `Tgene`, `Tchr`, `Tbp`, `Tstrand` - Tail gene information\n",
    "- `5'-gene sequence (10Kb)` - 5' sequence (10,000 bp)\n",
    "- `3'-gene sequence (10Kb)` - 3' sequence (10,000 bp)\n",
    "\n",
    "**What it does:**\n",
    "1. Loads positive and negative CSV files\n",
    "2. Merges 5' + 3' sequences ‚Üí 20kb sequences\n",
    "3. Adds labels (1=positive, 0=negative)\n",
    "4. Combines and shuffles data\n",
    "5. Validates sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataPreparator class defined\n"
     ]
    }
   ],
   "source": [
    "class DataPreparator:\n",
    "    \"\"\"Prepares FusionAI CSV data for HyenaDNA training\"\"\"\n",
    "    COLUMNS = [\"Hgene\",\"Hchr\",\"Hbp\",\"Hstrand\",\"Tgene\",\"Tchr\",\"Tbp\",\"Tstrand\",\"5'-gene sequence (10Kb)\",\"3'-gene sequence (10Kb)\"]\n",
    "    def __init__(self, positive_csv_path: str, negative_csv_path: str):\n",
    "        self.positive_csv_path = positive_csv_path\n",
    "        self.negative_csv_path = negative_csv_path\n",
    "\n",
    "    def load_and_prepare_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load positive and negative CSV files, merge sequences, and shuffle\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: sequence (20kb), label (0/1)\n",
    "        \"\"\"\n",
    "        print(\"Loading positive breakpoint data...\")\n",
    "        positive_df = pd.read_csv(self.positive_csv_path, header=None, names=COLUMNS, sep='\\t')\n",
    "        \n",
    "        print(\"Loading negative breakpoint data...\")\n",
    "        negative_df = pd.read_csv(self.negative_csv_path, header=None, names=COLUMNS, sep='\\t')\n",
    "        \n",
    "        # Merge 5' and 3' sequences to create 20kb sequences\n",
    "        print(\"Merging 5' and 3' gene sequences...\")\n",
    "        positive_df['sequence'] = positive_df[\"5'-gene sequence (10Kb)\"] + positive_df[\"3'-gene sequence (10Kb)\"]\n",
    "        positive_df['label'] = 1  # Positive breakpoint\n",
    "        \n",
    "        negative_df['sequence'] = negative_df[\"5'-gene sequence (10Kb)\"] + negative_df[\"3'-gene sequence (10Kb)\"]\n",
    "        negative_df['label'] = 0  # Negative breakpoint\n",
    "        \n",
    "        # Keep only sequence and label columns\n",
    "        positive_prepared = positive_df[['sequence', 'label']]\n",
    "        negative_prepared = negative_df[['sequence', 'label']]\n",
    "        \n",
    "        # Merge datasets\n",
    "        print(\"Merging positive and negative datasets...\")\n",
    "        combined_df = pd.concat([positive_prepared, negative_prepared], ignore_index=True)\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        print(\"Shuffling dataset...\")\n",
    "        combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # Validate sequences\n",
    "        print(\"Validating sequences...\")\n",
    "        self._validate_sequences(combined_df)\n",
    "        \n",
    "        print(f\"\\nDataset prepared:\")\n",
    "        print(f\"  Total samples: {len(combined_df)}\")\n",
    "        print(f\"  Positive samples: {(combined_df['label'] == 1).sum()}\")\n",
    "        print(f\"  Negative samples: {(combined_df['label'] == 0).sum()}\")\n",
    "        print(f\"  Average sequence length: {combined_df['sequence'].str.len().mean():.0f} bp\")\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def _validate_sequences(self, df: pd.DataFrame):\n",
    "        \"\"\"Validate DNA sequences\"\"\"\n",
    "        valid_bases = set('ATCGN')\n",
    "        \n",
    "        for idx, seq in enumerate(df['sequence'].head(100)):  # Check first 100\n",
    "            if not set(seq.upper()).issubset(valid_bases):\n",
    "                invalid_chars = set(seq.upper()) - valid_bases\n",
    "                print(f\"Warning: Invalid characters found in sequence {idx}: {invalid_chars}\")\n",
    "        \n",
    "        # Check for empty sequences\n",
    "        empty_sequences = df['sequence'].str.len() == 0\n",
    "        if empty_sequences.any():\n",
    "            print(f\"Warning: {empty_sequences.sum()} empty sequences found\")\n",
    "        \n",
    "        # Check sequence length distribution\n",
    "        seq_lengths = df['sequence'].str.len()\n",
    "        print(f\"  Sequence length range: {seq_lengths.min()} - {seq_lengths.max()} bp\")\n",
    "\n",
    "print(\"‚úÖ DataPreparator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 2: PyTorch Dataset Class\n",
    "\n",
    "Custom PyTorch Dataset for DNA sequences. This handles:\n",
    "- Tokenizing DNA sequences (character-level: A, T, C, G)\n",
    "- Padding/truncating to max length\n",
    "- Converting to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DNABreakpointDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class DNABreakpointDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for DNA sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: List[str], labels: List[int], tokenizer, max_length: int = 20480):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx].upper()\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the sequence\n",
    "        # HyenaDNA uses character-level tokenization (A, T, C, G)\n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            # 'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DNABreakpointDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Step 3: HyenaDNA Binary Classifier Model\n",
    "\n",
    "This defines the model architecture:\n",
    "\n",
    "**Architecture:**\n",
    "1. **HyenaDNA Backbone** (pretrained) - Extracts features from DNA sequences\n",
    "2. **Global Average Pooling** - Aggregates sequence representation\n",
    "3. **Classification Head:**\n",
    "   - Linear layer (hidden_size ‚Üí 512) + ReLU + Dropout\n",
    "   - Linear layer (512 ‚Üí 128) + ReLU + Dropout\n",
    "   - Linear layer (128 ‚Üí 2) for binary classification\n",
    "\n",
    "**Output:** Logits for 2 classes (negative=0, positive=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HyenaDNAClassifier class defined\n"
     ]
    }
   ],
   "source": [
    "class HyenaDNAClassifier(nn.Module):\n",
    "    \"\"\"HyenaDNA with binary classification head\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"LongSafari/hyenadna-tiny-1k-seqlen\", num_labels: int = 2):\n",
    "        super(HyenaDNAClassifier, self).__init__()\n",
    "        \n",
    "        print(f\"Loading HyenaDNA model: {model_name}\")\n",
    "        \n",
    "        # Load the pretrained HyenaDNA model\n",
    "        self.hyenadna = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Get the hidden size from the model config\n",
    "        self.hidden_size = self.hyenadna.config.d_model\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 256),  \n",
    "            nn.LayerNorm(256),              \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),                   \n",
    "            nn.Linear(256, num_labels)         \n",
    "        )\n",
    "        \n",
    "        print(f\"Model loaded. Hidden size: {self.hidden_size}\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get HyenaDNA embeddings\n",
    "        outputs = self.hyenadna(input_ids)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        # Take the mean over the sequence length dimension\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "        else:\n",
    "            sequence_output = outputs[0]\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled_output = torch.mean(sequence_output, dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        logits = torch.clamp(logits, min=-10, max=10)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ HyenaDNAClassifier class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 4: Metrics Function\n",
    "\n",
    "Computes evaluation metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Of predicted positives, how many are truly positive?\n",
    "- **Recall**: Of actual positives, how many did we find?\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **AUC**: Area under ROC curve (discrimination ability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics function defined\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute classification metrics\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    # For AUC, we need probabilities\n",
    "    probs = torch.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1)[:, 1].numpy()\n",
    "    probs = np.nan_to_num(probs, nan=0.5)\n",
    "\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    else:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metrics function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Training Pipeline Class\n",
    "\n",
    "Complete training pipeline that handles:\n",
    "- Dataset preparation and splitting\n",
    "- Model initialization\n",
    "- Training with Hugging Face Trainer\n",
    "- Model saving and checkpointing\n",
    "- Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HyenaDNATrainingPipeline class defined\n"
     ]
    }
   ],
   "source": [
    "class HyenaDNATrainingPipeline:\n",
    "    \"\"\"Complete training pipeline for HyenaDNA breakpoint classifier\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        output_dir: str,\n",
    "        max_length: int\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_datasets(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DNABreakpointDataset, DNABreakpointDataset]:\n",
    "        \"\"\"Prepare train and validation datasets\"\"\"\n",
    "        \n",
    "        # Split data\n",
    "        train_sequences, val_sequences, train_labels, val_labels = train_test_split(\n",
    "            train_df['sequence'].tolist(),\n",
    "            train_df['label'].tolist(),\n",
    "            test_size=val_split,\n",
    "            random_state=42,\n",
    "            stratify=train_df['label']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset split:\")\n",
    "        print(f\"  Training samples: {len(train_sequences)}\")\n",
    "        print(f\"  Validation samples: {len(val_sequences)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DNABreakpointDataset(\n",
    "            train_sequences, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        val_dataset = DNABreakpointDataset(\n",
    "            val_sequences, val_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_dataset: DNABreakpointDataset,\n",
    "        val_dataset: DNABreakpointDataset,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        warmup_steps: int,\n",
    "        weight_decay: float\n",
    "    ):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = HyenaDNAClassifier(self.model_name, num_labels=2)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=weight_decay,        \n",
    "            max_grad_norm=1.0,             \n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            bf16=use_bf16,\n",
    "            fp16=False, \n",
    "            logging_dir=f'{self.output_dir}/logs',\n",
    "            logging_steps=50,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            dataloader_num_workers=4,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\",\n",
    "            save_safetensors=False,\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Starting training...\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"\\nSaving final model...\")\n",
    "        save_model(self.model, f\"{self.output_dir}/final_model/model.safetensors\")\n",
    "        self.tokenizer.save_pretrained(f\"{self.output_dir}/final_model\")\n",
    "        \n",
    "        print(f\"Model saved to {self.output_dir}/final_model\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        test_csv_path: str,\n",
    "        model_path: str = None,\n",
    "        output_path: str = \"predictions.csv\",\n",
    "        batch_size: int = 8\n",
    "    ):\n",
    "        \"\"\"Make predictions on test data\"\"\"\n",
    "        \n",
    "        if model_path is None:\n",
    "            model_path = f\"{self.output_dir}/final_model\"\n",
    "        \n",
    "        \n",
    "        if self.model == None:\n",
    "            print(f\"\\nLoading model from {model_path}...\")\n",
    "            self.model = HyenaDNAClassifier(self.model_name, num_labels=2)\n",
    "            load_model(self.model, f\"{model_path}/model.safetensors\")\n",
    "            self.model.to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(f\"{OUTPUT_DIR}/final_model\",\n",
    "            trust_remote_code=True)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_csv_path}...\")\n",
    "        test_df = pd.read_csv(test_csv_path, header=None, names=COLUMNS, sep='\\t')\n",
    "        \n",
    "        # Prepare sequences\n",
    "        test_df['sequence'] = test_df[\"5'-gene sequence (10Kb)\"] + test_df[\"3'-gene sequence (10Kb)\"]\n",
    "        \n",
    "        # Create dataset (dummy labels)\n",
    "        test_dataset = DNABreakpointDataset(\n",
    "            test_df['sequence'].tolist(),\n",
    "            [0] * len(test_df),  # Dummy labels\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "        \n",
    "        # Create dataloader\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                # attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids)\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                # Get probabilities\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(probs, dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of positive class\n",
    "        \n",
    "        # Add predictions to dataframe\n",
    "        test_df['predicted_label'] = all_predictions\n",
    "        test_df['breakpoint_probability'] = all_probabilities\n",
    "        test_df['prediction'] = test_df['predicted_label'].map({0: 'Negative', 1: 'Positive'})\n",
    "        \n",
    "        # Save results\n",
    "        print(f\"\\nSaving predictions to {output_path}...\")\n",
    "        test_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(\"\\nPrediction Summary:\")\n",
    "        print(f\"  Total samples: {len(test_df)}\")\n",
    "        print(f\"  Predicted positive: {(test_df['predicted_label'] == 1).sum()}\")\n",
    "        print(f\"  Predicted negative: {(test_df['predicted_label'] == 0).sum()}\")\n",
    "        print(f\"  Mean positive probability: {test_df['breakpoint_probability'].mean():.4f}\")\n",
    "        \n",
    "        return test_df\n",
    "\n",
    "print(\"‚úÖ HyenaDNATrainingPipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üöÄ EXECUTION STARTS HERE\n",
    "\n",
    "Now we'll run the complete pipeline step by step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 6: Load and Prepare Data\n",
    "\n",
    "Load the positive and negative CSV files, merge sequences, and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: DATA PREPARATION\n",
      "======================================================================\n",
      "Loading positive breakpoint data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading negative breakpoint data...\n",
      "Merging 5' and 3' gene sequences...\n",
      "Merging positive and negative datasets...\n",
      "Shuffling dataset...\n",
      "Validating sequences...\n",
      "  Sequence length range: 20000 - 20000 bp\n",
      "\n",
      "Dataset prepared:\n",
      "  Total samples: 50745\n",
      "  Positive samples: 30745\n",
      "  Negative samples: 20000\n",
      "  Average sequence length: 20000 bp\n",
      "\n",
      "First 3 samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCTGGGATTACAGGTGCCCACCACCATGCCTGGCTAATTTTTGTAT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCTCAGCCCTCCCATACAATTCTCCCAATGATAAGTGTGAGAACAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTGCACTCAAGCTATCCCCCCACCTCAGCCTCCCAAAGAGCTGGG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  label\n",
       "0  GCTGGGATTACAGGTGCCCACCACCATGCCTGGCTAATTTTTGTAT...      1\n",
       "1  CCTCAGCCCTCCCATACAATTCTCCCAATGATAAGTGTGAGAACAC...      0\n",
       "2  CCTGCACTCAAGCTATCCCCCCACCTCAGCCTCCCAAAGAGCTGGG...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 1: DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize data preparator\n",
    "data_prep = DataPreparator(POSITIVE_CSV, NEGATIVE_CSV)\n",
    "\n",
    "# Load and prepare data\n",
    "train_df = data_prep.load_and_prepare_data()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "display(train_df.head(3))\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 7: Initialize Training Pipeline\n",
    "\n",
    "Create the training pipeline with the specified model and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 2: INITIALIZE TRAINING PIPELINE\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-medium-160k-seqlen-hf:\n",
      "- tokenization_hyena.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Pipeline initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2: INITIALIZE TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pipeline = HyenaDNATrainingPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Prepare Training and Validation Datasets\n",
    "\n",
    "Split the data into training and validation sets (80/20 split by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 3: PREPARE TRAINING AND VALIDATION DATASETS\n",
      "======================================================================\n",
      "\n",
      "Dataset split:\n",
      "  Training samples: 40596\n",
      "  Validation samples: 10149\n",
      "\n",
      "Training samples: 40596\n",
      "Validation samples: 10149\n",
      "\n",
      "‚úÖ Datasets prepared!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 3: PREPARE TRAINING AND VALIDATION DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_dataset, val_dataset = pipeline.prepare_datasets(train_df, val_split=VAL_SPLIT)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(\"\\n‚úÖ Datasets prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 9: Train the Model\n",
    "\n",
    "This is the main training step. It will:\n",
    "- Load the pretrained HyenaDNA model\n",
    "- Add the classification head\n",
    "- Fine-tune on your data\n",
    "- Evaluate on validation set periodically\n",
    "- Save the best model based on F1 score\n",
    "\n",
    "**Expected time:** 30 minutes to several hours depending on:\n",
    "- Dataset size\n",
    "- GPU type (T4 vs A100)\n",
    "- Number of epochs\n",
    "- Model size\n",
    "\n",
    "**Progress indicators:**\n",
    "- Training loss per batch\n",
    "- Validation metrics every 200 steps\n",
    "- Best model automatically saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: TRAIN THE MODEL\n",
      "======================================================================\n",
      "\n",
      "Training configuration:\n",
      "  Epochs: 3\n",
      "  Batch size: 8\n",
      "  Learning rate: 2e-05\n",
      "  Warmup steps: 1000\n",
      "  Weight decay: 0.01\n",
      "\n",
      "This may take a while... ‚òï\n",
      "\n",
      "Loading HyenaDNA model: LongSafari/hyenadna-medium-160k-seqlen-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-medium-160k-seqlen-hf:\n",
      "- configuration_hyena.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-medium-160k-seqlen-hf:\n",
      "- modeling_hyena.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Hidden size: 256\n",
      "\n",
      "==================================================\n",
      "Starting training...\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15225' max='15225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15225/15225 2:35:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.543200</td>\n",
       "      <td>0.536436</td>\n",
       "      <td>0.766282</td>\n",
       "      <td>0.763720</td>\n",
       "      <td>0.889413</td>\n",
       "      <td>0.821788</td>\n",
       "      <td>0.830335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.534729</td>\n",
       "      <td>0.772096</td>\n",
       "      <td>0.772753</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.824520</td>\n",
       "      <td>0.850227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.547900</td>\n",
       "      <td>0.499599</td>\n",
       "      <td>0.799685</td>\n",
       "      <td>0.804889</td>\n",
       "      <td>0.883558</td>\n",
       "      <td>0.842391</td>\n",
       "      <td>0.868315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.498878</td>\n",
       "      <td>0.800966</td>\n",
       "      <td>0.782383</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.849926</td>\n",
       "      <td>0.878564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.508190</td>\n",
       "      <td>0.804316</td>\n",
       "      <td>0.813055</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>0.844820</td>\n",
       "      <td>0.875451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>0.520130</td>\n",
       "      <td>0.785003</td>\n",
       "      <td>0.757163</td>\n",
       "      <td>0.949748</td>\n",
       "      <td>0.842591</td>\n",
       "      <td>0.881688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.494452</td>\n",
       "      <td>0.798798</td>\n",
       "      <td>0.838359</td>\n",
       "      <td>0.827452</td>\n",
       "      <td>0.832870</td>\n",
       "      <td>0.873166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.476290</td>\n",
       "      <td>0.815647</td>\n",
       "      <td>0.849510</td>\n",
       "      <td>0.845503</td>\n",
       "      <td>0.847502</td>\n",
       "      <td>0.887787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.497519</td>\n",
       "      <td>0.804611</td>\n",
       "      <td>0.790354</td>\n",
       "      <td>0.922101</td>\n",
       "      <td>0.851160</td>\n",
       "      <td>0.885358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.817420</td>\n",
       "      <td>0.813852</td>\n",
       "      <td>0.905838</td>\n",
       "      <td>0.857385</td>\n",
       "      <td>0.890923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.469109</td>\n",
       "      <td>0.819095</td>\n",
       "      <td>0.839874</td>\n",
       "      <td>0.866645</td>\n",
       "      <td>0.853049</td>\n",
       "      <td>0.892196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>0.469161</td>\n",
       "      <td>0.822446</td>\n",
       "      <td>0.852212</td>\n",
       "      <td>0.855261</td>\n",
       "      <td>0.853734</td>\n",
       "      <td>0.894239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.547877</td>\n",
       "      <td>0.766381</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.688730</td>\n",
       "      <td>0.781293</td>\n",
       "      <td>0.881928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.504752</td>\n",
       "      <td>0.791408</td>\n",
       "      <td>0.883708</td>\n",
       "      <td>0.755082</td>\n",
       "      <td>0.814347</td>\n",
       "      <td>0.882612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.473707</td>\n",
       "      <td>0.816238</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>0.899821</td>\n",
       "      <td>0.855773</td>\n",
       "      <td>0.891363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.475240</td>\n",
       "      <td>0.815844</td>\n",
       "      <td>0.796481</td>\n",
       "      <td>0.934949</td>\n",
       "      <td>0.860178</td>\n",
       "      <td>0.898556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>0.471576</td>\n",
       "      <td>0.822347</td>\n",
       "      <td>0.851050</td>\n",
       "      <td>0.856725</td>\n",
       "      <td>0.853878</td>\n",
       "      <td>0.895358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.465337</td>\n",
       "      <td>0.826190</td>\n",
       "      <td>0.856446</td>\n",
       "      <td>0.856725</td>\n",
       "      <td>0.856585</td>\n",
       "      <td>0.899512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.462398</td>\n",
       "      <td>0.826781</td>\n",
       "      <td>0.857166</td>\n",
       "      <td>0.856887</td>\n",
       "      <td>0.857027</td>\n",
       "      <td>0.899638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.459835</td>\n",
       "      <td>0.830131</td>\n",
       "      <td>0.833057</td>\n",
       "      <td>0.899984</td>\n",
       "      <td>0.865228</td>\n",
       "      <td>0.898829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.458512</td>\n",
       "      <td>0.830821</td>\n",
       "      <td>0.857766</td>\n",
       "      <td>0.864043</td>\n",
       "      <td>0.860893</td>\n",
       "      <td>0.904791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>0.459306</td>\n",
       "      <td>0.832791</td>\n",
       "      <td>0.839019</td>\n",
       "      <td>0.895918</td>\n",
       "      <td>0.866536</td>\n",
       "      <td>0.904986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.459137</td>\n",
       "      <td>0.830919</td>\n",
       "      <td>0.863182</td>\n",
       "      <td>0.856725</td>\n",
       "      <td>0.859941</td>\n",
       "      <td>0.903914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>0.459301</td>\n",
       "      <td>0.834762</td>\n",
       "      <td>0.872791</td>\n",
       "      <td>0.851358</td>\n",
       "      <td>0.861941</td>\n",
       "      <td>0.904785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.453199</td>\n",
       "      <td>0.834269</td>\n",
       "      <td>0.847194</td>\n",
       "      <td>0.886323</td>\n",
       "      <td>0.866317</td>\n",
       "      <td>0.906762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.450605</td>\n",
       "      <td>0.836536</td>\n",
       "      <td>0.851991</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.867566</td>\n",
       "      <td>0.907616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>0.451456</td>\n",
       "      <td>0.835944</td>\n",
       "      <td>0.854747</td>\n",
       "      <td>0.878517</td>\n",
       "      <td>0.866469</td>\n",
       "      <td>0.907976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.397200</td>\n",
       "      <td>0.451990</td>\n",
       "      <td>0.836437</td>\n",
       "      <td>0.853855</td>\n",
       "      <td>0.880794</td>\n",
       "      <td>0.867115</td>\n",
       "      <td>0.908349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>0.452966</td>\n",
       "      <td>0.836634</td>\n",
       "      <td>0.849930</td>\n",
       "      <td>0.886973</td>\n",
       "      <td>0.868057</td>\n",
       "      <td>0.908377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.452818</td>\n",
       "      <td>0.836043</td>\n",
       "      <td>0.848919</td>\n",
       "      <td>0.887299</td>\n",
       "      <td>0.867684</td>\n",
       "      <td>0.908355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final model...\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: I/O error: No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThis may take a while... ‚òï\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m trainer = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mHyenaDNATrainingPipeline.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, num_epochs, batch_size, learning_rate, warmup_steps, weight_decay)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSaving final model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/final_model/model.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer.save_pretrained(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/final_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/final_model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GeneAI/venv/lib/python3.12/site-packages/safetensors/torch.py:183\u001b[39m, in \u001b[36msave_model\u001b[39m\u001b[34m(model, filename, metadata, force_contiguous)\u001b[39m\n\u001b[32m    181\u001b[39m     state_dict = {k: v.contiguous() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict.items()}\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    185\u001b[39m     msg = \u001b[38;5;28mstr\u001b[39m(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GeneAI/venv/lib/python3.12/site-packages/safetensors/torch.py:352\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    322\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    323\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    324\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    325\u001b[39m ):\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    328\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: I/O error: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 4: TRAIN THE MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\nThis may take a while... ‚òï\")\n",
    "print()\n",
    "\n",
    "trainer = pipeline.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 10: View Training Results\n",
    "\n",
    "Display the final training metrics and model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get training history\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mtrainer\u001b[49m.state.log_history\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Extract validation metrics\u001b[39;00m\n\u001b[32m      5\u001b[39m val_metrics = [entry \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33meval_accuracy\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry]\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Get training history\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Extract validation metrics\n",
    "val_metrics = [entry for entry in history if 'eval_accuracy' in entry]\n",
    "\n",
    "if val_metrics:\n",
    "    best_metrics = val_metrics[-1]\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL VALIDATION METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy:  {best_metrics.get('eval_accuracy', 0):.4f}\")\n",
    "    print(f\"Precision: {best_metrics.get('eval_precision', 0):.4f}\")\n",
    "    print(f\"Recall:    {best_metrics.get('eval_recall', 0):.4f}\")\n",
    "    print(f\"F1 Score:  {best_metrics.get('eval_f1', 0):.4f}\")\n",
    "    print(f\"AUC:       {best_metrics.get('eval_auc', 0):.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to: {OUTPUT_DIR}/final_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Step 11: Make Predictions on Test Data\n",
    "\n",
    "Load the test CSV file and make predictions using the trained model.\n",
    "\n",
    "**Output:** CSV file with:\n",
    "- All original columns\n",
    "- `predicted_label` (0 or 1)\n",
    "- `breakpoint_probability` (0.0 to 1.0)\n",
    "- `prediction` (\"Negative\" or \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 5: MAKE PREDICTIONS ON TEST DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predictions_df = pipeline.predict(\n",
    "    test_csv_path=TEST_CSV,\n",
    "    output_path=\"breakpoint_predictions.csv\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Predictions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 12: View Prediction Results\n",
    "\n",
    "Display sample predictions and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTotal test samples: {len(predictions_df)}\")\n",
    "print(f\"Predicted Positive: {(predictions_df['predicted_label'] == 1).sum()} ({(predictions_df['predicted_label'] == 1).sum() / len(predictions_df) * 100:.1f}%)\")\n",
    "print(f\"Predicted Negative: {(predictions_df['predicted_label'] == 0).sum()} ({(predictions_df['predicted_label'] == 0).sum() / len(predictions_df) * 100:.1f}%)\")\n",
    "print(f\"\\nAverage breakpoint probability: {predictions_df['breakpoint_probability'].mean():.4f}\")\n",
    "print(f\"Min probability: {predictions_df['breakpoint_probability'].min():.4f}\")\n",
    "print(f\"Max probability: {predictions_df['breakpoint_probability'].max():.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS (first 5 rows)\")\n",
    "print(\"=\"*70)\n",
    "display(predictions_df[['predicted_label', 'breakpoint_probability', 'prediction']].head())\n",
    "\n",
    "# Probability distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROBABILITY DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(predictions_df['breakpoint_probability'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä (Optional) Step 13: Visualize Results\n",
    "\n",
    "Create visualizations of the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Prediction distribution\n",
    "predictions_df['prediction'].value_counts().plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Probability distribution\n",
    "axes[1].hist(predictions_df['breakpoint_probability'], bins=30, edgecolor='black', alpha=0.7, color='#95E1D3')\n",
    "axes[1].set_title('Breakpoint Probability Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Box plot by prediction\n",
    "predictions_df.boxplot(column='breakpoint_probability', by='prediction', ax=axes[2])\n",
    "axes[2].set_title('Probability by Prediction', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Prediction')\n",
    "axes[2].set_ylabel('Breakpoint Probability')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved as 'prediction_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 14: Save/Download Results\n",
    "\n",
    "Download the results if you're using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Uncomment these lines to download files\n",
    "# from google.colab import files\n",
    "# \n",
    "# # Download predictions\n",
    "# files.download('breakpoint_predictions.csv')\n",
    "# \n",
    "# # Download visualization (if created)\n",
    "# files.download('prediction_analysis.png')\n",
    "# \n",
    "# # Optionally download the model\n",
    "# !zip -r hyenadna_model.zip hyenadna_breakpoint_model/final_model/\n",
    "# files.download('hyenadna_model.zip')\n",
    "\n",
    "print(\"\\nFiles ready for download:\")\n",
    "print(\"  - breakpoint_predictions.csv\")\n",
    "print(\"  - prediction_analysis.png (if visualization was run)\")\n",
    "print(f\"  - {OUTPUT_DIR}/final_model/ (trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ PIPELINE COMPLETE!\n",
    "\n",
    "## What You've Accomplished:\n",
    "\n",
    "1. ‚úÖ Loaded and prepared FusionAI format data\n",
    "2. ‚úÖ Merged 5' and 3' sequences into 20kb inputs\n",
    "3. ‚úÖ Fine-tuned HyenaDNA for binary classification\n",
    "4. ‚úÖ Evaluated model performance on validation data\n",
    "5. ‚úÖ Made predictions on test data\n",
    "6. ‚úÖ Saved results and visualizations\n",
    "\n",
    "## Output Files:\n",
    "\n",
    "- **`breakpoint_predictions.csv`** - Test predictions with probabilities\n",
    "- **`hyenadna_breakpoint_model/final_model/`** - Trained model weights\n",
    "- **`prediction_analysis.png`** - Visualization (if created)\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Analyze results** - Check prediction quality and metrics\n",
    "2. **Adjust hyperparameters** - Try different learning rates, batch sizes, or epochs\n",
    "3. **Use the model** - Apply to new data using the saved model\n",
    "4. **Experiment** - Try different HyenaDNA model sizes for better performance\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ To Use the Trained Model Later:\n",
    "\n",
    "```python\n",
    "# Load the pipeline\n",
    "pipeline = HyenaDNATrainingPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_predictions = pipeline.predict(\n",
    "    test_csv_path=\"new_test_data.csv\",\n",
    "    model_path=\"./hyenadna_breakpoint_model/final_model\",\n",
    "    output_path=\"new_predictions.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the comments in each cell or refer to the README documentation.\n",
    "\n",
    "**Happy predicting! üß¨üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
