{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyenaDNA Binary Classifier for DNA Breakpoint Detection\n",
    "\n",
    "This notebook implements a complete pipeline to fine-tune HyenaDNA for classifying DNA sequences as positive or negative breakpoints, similar to FusionAI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**What this notebook does:**\n",
    "1. **Prepares data** from FusionAI-format CSV files (positive/negative breakpoints)\n",
    "2. **Merges sequences** (5' + 3' = 20kb input)\n",
    "3. **Fine-tunes HyenaDNA** with a binary classification head\n",
    "4. **Makes predictions** on test data\n",
    "\n",
    "**Key Features:**\n",
    "- Handles sequences up to 1M bp (we'll use 20kb)\n",
    "- Uses Hugging Face Trainer for optimization\n",
    "- Automatic mixed precision (FP16) for faster training\n",
    "- Comprehensive metrics (Accuracy, Precision, Recall, F1, AUC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup\n",
    "\n",
    "First, let's install all required dependencies and check our compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -q torch transformers accelerate pandas scikit-learn\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and setup\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Consider using a GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "from safetensors.torch import load_model, save_model\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Set all hyperparameters and file paths here. **Update the CSV paths to match your data!**\n",
    "\n",
    "### Model Selection Guide:\n",
    "- `hyenadna-tiny-1k-seqlen` - Max 1kb (‚ùå Too short for 20kb!)\n",
    "- `hyenadna-small-32k-seqlen` - Max 32kb (‚úÖ **Recommended for 20kb**)\n",
    "- `hyenadna-medium-160k-seqlen` - Max 160kb (Better performance, more memory)\n",
    "- `hyenadna-large-1m-seqlen` - Max 1M (Best performance, requires A100 80GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ FILE PATHS - UPDATE THESE! ============\n",
    "POSITIVE_CSV = \"datasets/fusion_gene_positive_bp_information_with_class_for_modeling.txt\"  # Path to positive breakpoint CSV\n",
    "NEGATIVE_CSV = \"datasets/fusion_gene_negative_bp_information_with_class_for_modeling.txt\"  # Path to negative breakpoint CSV\n",
    "TEST_CSV = \"datasets/fusion_gene_positive_bp_information_with_class_for_testing.txt\"          # Path to test data CSV\n",
    "COLUMNS = [\"Hgene\",\"Hchr\",\"Hbp\",\"Hstrand\",\"Tgene\",\"Tchr\",\"Tbp\",\"Tstrand\",\"5'-gene sequence (10Kb)\",\"3'-gene sequence (10Kb)\"] # CSV Columns names\n",
    "OUTPUT_DIR = \"./hyenadna_breakpoint_model\" # Where to save the model\n",
    "\n",
    "# ============ MODEL CONFIGURATION ============\n",
    "MODEL_NAME = \"LongSafari/hyenadna-small-32k-seqlen-hf\"  # Recommended for 20kb sequences\n",
    "MAX_LENGTH = 20480  # Maximum sequence length (20kb)\n",
    "\n",
    "# ============ TRAINING HYPERPARAMETERS ============\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8          # Adjust based on GPU memory (2 for T4, 4 for V100, 8 for A100)\n",
    "LEARNING_RATE = 1e-5    # Learning rate\n",
    "WARMUP_STEPS = 1000      # Learning rate warmup\n",
    "WEIGHT_DECAY = 0.01     # L2 regularization\n",
    "VAL_SPLIT = 0.2         # Validation split (20%)\n",
    "\n",
    "# ============ DEVICE SETUP ============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Preparation Class\n",
    "\n",
    "This class handles loading and preparing the FusionAI CSV files.\n",
    "\n",
    "**Input CSV Format (tab-separated):**\n",
    "- `Hgene`, `Hchr`, `Hbp`, `Hstrand` - Head gene information\n",
    "- `Tgene`, `Tchr`, `Tbp`, `Tstrand` - Tail gene information\n",
    "- `5'-gene sequence (10Kb)` - 5' sequence (10,000 bp)\n",
    "- `3'-gene sequence (10Kb)` - 3' sequence (10,000 bp)\n",
    "\n",
    "**What it does:**\n",
    "1. Loads positive and negative CSV files\n",
    "2. Merges 5' + 3' sequences ‚Üí 20kb sequences\n",
    "3. Adds labels (1=positive, 0=negative)\n",
    "4. Combines and shuffles data\n",
    "5. Validates sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparator:\n",
    "    \"\"\"\n",
    "    Prepares FusionAI CSV data for HyenaDNA training with Data Augmentation\n",
    "    \"\"\"\n",
    "    COLUMNS = [\"Hgene\",\"Hchr\",\"Hbp\",\"Hstrand\",\"Tgene\",\"Tchr\",\"Tbp\",\"Tstrand\",\"5'-gene sequence (10Kb)\",\"3'-gene sequence (10Kb)\"]\n",
    "\n",
    "    def __init__(self, positive_csv_path: str, negative_csv_path: str, extra_fasta_path: str = \"datasets/chimeras_43466.fa\"):\n",
    "        self.positive_csv_path = positive_csv_path\n",
    "        self.negative_csv_path = negative_csv_path\n",
    "        self.extra_fasta_path = extra_fasta_path\n",
    "        # Translation table for efficient reverse complement calculation\n",
    "        self.trans_table = str.maketrans(\"ATCGN\", \"TAGCN\")\n",
    "\n",
    "    def _get_reverse_complement(self, sequence: str) -> str:\n",
    "        \"\"\"\n",
    "        Returns the reverse complement of a DNA sequence.\n",
    "        Fast implementation using string translation.\n",
    "        \"\"\"\n",
    "        # 1. Translate (A->T, C->G, etc.)\n",
    "        # 2. Reverse string ([::-1])\n",
    "        return sequence.upper().translate(self.trans_table)[::-1]\n",
    "\n",
    "    def load_and_prepare_data(self, augment: bool = False) -> pd.DataFrame:\n",
    "        print(\"Loading positive breakpoint data...\")\n",
    "        positive_df = pd.read_csv(self.positive_csv_path, header=None, names=self.COLUMNS, sep='\\t')\n",
    "\n",
    "        print(\"Loading negative breakpoint data...\")\n",
    "        negative_df = pd.read_csv(self.negative_csv_path, header=None, names=self.COLUMNS, sep='\\t')\n",
    "\n",
    "        # Merge 5' and 3' sequences\n",
    "        print(\"Merging 5' and 3' gene sequences...\")\n",
    "        positive_df['sequence'] = positive_df[\"5'-gene sequence (10Kb)\"] + positive_df[\"3'-gene sequence (10Kb)\"]\n",
    "        positive_df['label'] = 1\n",
    "\n",
    "        negative_df['sequence'] = negative_df[\"5'-gene sequence (10Kb)\"] + negative_df[\"3'-gene sequence (10Kb)\"]\n",
    "        negative_df['label'] = 0\n",
    "\n",
    "        # Reduce to sequence + label\n",
    "        positive_prepared = positive_df[['sequence', 'label']]\n",
    "        negative_prepared = negative_df[['sequence', 'label']]\n",
    "\n",
    "        # Load additional positive sequences from FASTA\n",
    "        print(f\"Loading extra positive samples from: {self.extra_fasta_path}\")\n",
    "        if os.path.exists(self.extra_fasta_path):\n",
    "            extra_sequences = self._load_fasta_multiline(self.extra_fasta_path)\n",
    "            print(f\"  Loaded {len(extra_sequences)} FASTA sequences\")\n",
    "            extra_positive_df = pd.DataFrame({\n",
    "                \"sequence\": extra_sequences,\n",
    "                \"label\": 1\n",
    "            })\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Warning: Extra FASTA file not found. Skipping.\")\n",
    "            extra_positive_df = pd.DataFrame(columns=['sequence', 'label'])\n",
    "\n",
    "        # Merge all datasets\n",
    "        print(\"Merging datasets (original positives + FASTA positives + negatives)...\")\n",
    "        combined_df = pd.concat([positive_prepared, extra_positive_df, negative_prepared], ignore_index=True)\n",
    "\n",
    "        # --- AUGMENTATION LOGIC START ---\n",
    "        if augment:\n",
    "            print(f\"\\nApplying Reverse Complement Augmentation...\")\n",
    "            print(f\"  Original count: {len(combined_df)}\")\n",
    "            \n",
    "            # Create augmented copy\n",
    "            augmented_df = combined_df.copy()\n",
    "            # Apply reverse complement to the sequence column\n",
    "            augmented_df['sequence'] = augmented_df['sequence'].apply(self._get_reverse_complement)\n",
    "            \n",
    "            # Concatenate original + augmented\n",
    "            combined_df = pd.concat([combined_df, augmented_df], ignore_index=True)\n",
    "            print(f\"  Augmented count: {len(combined_df)} (Doubled dataset)\")\n",
    "        # --- AUGMENTATION LOGIC END ---\n",
    "\n",
    "        # Shuffle\n",
    "        print(\"Shuffling dataset...\")\n",
    "        combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Validate\n",
    "        print(\"Validating sequences...\")\n",
    "        self._validate_sequences(combined_df)\n",
    "\n",
    "        print(f\"\\nDataset prepared:\")\n",
    "        print(f\"  Total samples: {len(combined_df)}\")\n",
    "        print(f\"  Positive samples: {(combined_df['label'] == 1).sum()}\")\n",
    "        print(f\"  Negative samples: {(combined_df['label'] == 0).sum()}\")\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def _load_fasta_multiline(self, fasta_path: str):\n",
    "        \"\"\"Loads sequences from multi-line FASTA.\"\"\"\n",
    "        sequences = []\n",
    "        current_seq = []\n",
    "        with open(fasta_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.startswith(\">\"):\n",
    "                    if current_seq:\n",
    "                        sequences.append(\"\".join(current_seq))\n",
    "                        current_seq = []\n",
    "                else:\n",
    "                    current_seq.append(line)\n",
    "        if current_seq:\n",
    "            sequences.append(\"\".join(current_seq))\n",
    "        return sequences\n",
    "\n",
    "    def _validate_sequences(self, df: pd.DataFrame):\n",
    "        valid_bases = set('ATCGN')\n",
    "        # Quick check on a few samples\n",
    "        for idx, seq in enumerate(df['sequence'].head(20)):\n",
    "            if not set(seq.upper()).issubset(valid_bases):\n",
    "                print(f\"Warning: Invalid characters in sequence {idx}\")\n",
    "                break\n",
    "\n",
    "print(\"‚úÖ DataPreparator class updated with Reverse Complement augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 2: PyTorch Dataset Class\n",
    "\n",
    "Custom PyTorch Dataset for DNA sequences. This handles:\n",
    "- Tokenizing DNA sequences (character-level: A, T, C, G)\n",
    "- Padding/truncating to max length\n",
    "- Converting to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNABreakpointDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for DNA sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: List[str], labels: List[int], tokenizer, max_length: int = 20480):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx].upper()\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the sequence\n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        # HyenaDNATokenizer does not return attention_mask, so we create it:\n",
    "        if 'attention_mask' in encoding:\n",
    "            attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        else:\n",
    "            pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 4\n",
    "            attention_mask = (input_ids != pad_token_id).long()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DNABreakpointDataset class defined (attention_mask patch applied)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Step 3: HyenaDNA Binary Classifier Model\n",
    "\n",
    "This defines the model architecture:\n",
    "\n",
    "**Architecture:**\n",
    "1. **HyenaDNA Backbone** (pretrained) - Extracts features from DNA sequences\n",
    "2. **Global Average Pooling** - Aggregates sequence representation\n",
    "3. **Classification Head:**\n",
    "   - Linear layer (hidden_size ‚Üí 512) + ReLU + Dropout\n",
    "   - Linear layer (512 ‚Üí 128) + ReLU + Dropout\n",
    "   - Linear layer (128 ‚Üí 2) for binary classification\n",
    "\n",
    "**Output:** Logits for 2 classes (negative=0, positive=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyenaDNAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    HyenaDNA with Enhanced Pooling (Mean + Max) for Breakpoint Detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"LongSafari/hyenadna-small-32k-seqlen-hf\", num_labels: int = 2):\n",
    "        super(HyenaDNAClassifier, self).__init__()\n",
    "        \n",
    "        print(f\"Loading HyenaDNA model: {model_name}\")\n",
    "        \n",
    "        # Load the pretrained HyenaDNA model\n",
    "        self.hyenadna = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Get the hidden size from the model config\n",
    "        self.hidden_size = self.hyenadna.config.d_model\n",
    "        \n",
    "        # --- IMPROVEMENT START ---\n",
    "        # We are now using Mean + Max pooling, so the input dimension doubles\n",
    "        classifier_input_dim = self.hidden_size * 2\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 256), \n",
    "            nn.LayerNorm(256),              \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),                   \n",
    "            nn.Linear(256, num_labels)         \n",
    "        )\n",
    "        # --- IMPROVEMENT END ---\n",
    "        \n",
    "        print(f\"Model loaded. Hidden size: {self.hidden_size}\")\n",
    "        print(f\"Classifier input dimension: {classifier_input_dim} (Mean + Max Pooling)\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get HyenaDNA embeddings\n",
    "        outputs = self.hyenadna(input_ids)\n",
    "        \n",
    "        # Extract last hidden state\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "        else:\n",
    "            sequence_output = outputs[0]\n",
    "            \n",
    "        # --- IMPROVEMENT START: ENHANCED POOLING ---\n",
    "        # 1. Mean Pooling (General context)\n",
    "        # Shape: [batch_size, hidden_size]\n",
    "        mean_pool = torch.mean(sequence_output, dim=1)\n",
    "        \n",
    "        # 2. Max Pooling (Specific feature detection - crucial for breakpoints)\n",
    "        # torch.max returns (values, indices), we only need values\n",
    "        # Shape: [batch_size, hidden_size]\n",
    "        max_pool, _ = torch.max(sequence_output, dim=1)\n",
    "        \n",
    "        # 3. Concatenate\n",
    "        # Shape: [batch_size, hidden_size * 2]\n",
    "        pooled_output = torch.cat((mean_pool, max_pool), dim=1)\n",
    "        # --- IMPROVEMENT END ---\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        logits = torch.clamp(logits, min=-10, max=10)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Weighted loss can be added here if handling class imbalance\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ HyenaDNAClassifier updated with Mean+Max pooling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 4: Metrics Function\n",
    "\n",
    "Computes evaluation metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Of predicted positives, how many are truly positive?\n",
    "- **Recall**: Of actual positives, how many did we find?\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **AUC**: Area under ROC curve (discrimination ability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute classification metrics\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    # For AUC, we need probabilities\n",
    "    probs = torch.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1)[:, 1].numpy()\n",
    "    probs = np.nan_to_num(probs, nan=0.5)\n",
    "\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    else:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metrics function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Training Pipeline Class\n",
    "\n",
    "Complete training pipeline that handles:\n",
    "- Dataset preparation and splitting\n",
    "- Model initialization\n",
    "- Training with Hugging Face Trainer\n",
    "- Model saving and checkpointing\n",
    "- Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyenaDNATrainingPipeline:\n",
    "    \"\"\"Complete training pipeline for HyenaDNA breakpoint classifier\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        output_dir: str,\n",
    "        max_length: int\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_datasets(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DNABreakpointDataset, DNABreakpointDataset]:\n",
    "        \"\"\"Prepare train and validation datasets\"\"\"\n",
    "        \n",
    "        # Split data\n",
    "        train_sequences, val_sequences, train_labels, val_labels = train_test_split(\n",
    "            train_df['sequence'].tolist(),\n",
    "            train_df['label'].tolist(),\n",
    "            test_size=val_split,\n",
    "            random_state=42,\n",
    "            stratify=train_df['label']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset split:\")\n",
    "        print(f\"  Training samples: {len(train_sequences)}\")\n",
    "        print(f\"  Validation samples: {len(val_sequences)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DNABreakpointDataset(\n",
    "            train_sequences, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        val_dataset = DNABreakpointDataset(\n",
    "            val_sequences, val_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_dataset: DNABreakpointDataset,\n",
    "        val_dataset: DNABreakpointDataset,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        warmup_steps: int,\n",
    "        weight_decay: float\n",
    "    ):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = HyenaDNAClassifier(self.model_name, num_labels=2)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=weight_decay,        \n",
    "            max_grad_norm=1.0,             \n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            bf16=use_bf16,\n",
    "            fp16=False, \n",
    "            logging_dir=f'{self.output_dir}/logs',\n",
    "            logging_steps=50,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            dataloader_num_workers=4,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\",\n",
    "            save_safetensors=False,\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Starting training...\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"\\nSaving final model...\")\n",
    "        save_model(self.model, f\"{self.output_dir}/final_model/model.safetensors\")\n",
    "        self.tokenizer.save_pretrained(f\"{self.output_dir}/final_model\")\n",
    "        \n",
    "        print(f\"Model saved to {self.output_dir}/final_model\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        test_csv_path: str,\n",
    "        model_path: str = None,\n",
    "        output_path: str = \"predictions.csv\",\n",
    "        batch_size: int = 8\n",
    "    ):\n",
    "        \"\"\"Make predictions on test data\"\"\"\n",
    "        \n",
    "        if model_path is None:\n",
    "            model_path = f\"{self.output_dir}/final_model\"\n",
    "        \n",
    "        \n",
    "        if self.model == None:\n",
    "            print(f\"\\nLoading model from {model_path}...\")\n",
    "            self.model = HyenaDNAClassifier(self.model_name, num_labels=2)\n",
    "            load_model(self.model, f\"{model_path}/model.safetensors\")\n",
    "            self.model.to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(f\"{OUTPUT_DIR}/final_model\",\n",
    "            trust_remote_code=True)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_csv_path}...\")\n",
    "        test_df = pd.read_csv(test_csv_path, header=None, names=COLUMNS, sep='\\t')\n",
    "        \n",
    "        # Prepare sequences\n",
    "        test_df['sequence'] = test_df[\"5'-gene sequence (10Kb)\"] + test_df[\"3'-gene sequence (10Kb)\"]\n",
    "        \n",
    "        # Create dataset (dummy labels)\n",
    "        test_dataset = DNABreakpointDataset(\n",
    "            test_df['sequence'].tolist(),\n",
    "            [0] * len(test_df),  # Dummy labels\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "        \n",
    "        # Create dataloader\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids)\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                # Get probabilities\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(probs, dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of positive class\n",
    "        \n",
    "        # Add predictions to dataframe\n",
    "        test_df['predicted_label'] = all_predictions\n",
    "        test_df['breakpoint_probability'] = all_probabilities\n",
    "        test_df['prediction'] = test_df['predicted_label'].map({0: 'Negative', 1: 'Positive'})\n",
    "        \n",
    "        # Save results\n",
    "        print(f\"\\nSaving predictions to {output_path}...\")\n",
    "        test_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(\"\\nPrediction Summary:\")\n",
    "        print(f\"  Total samples: {len(test_df)}\")\n",
    "        print(f\"  Predicted positive: {(test_df['predicted_label'] == 1).sum()}\")\n",
    "        print(f\"  Predicted negative: {(test_df['predicted_label'] == 0).sum()}\")\n",
    "        print(f\"  Mean positive probability: {test_df['breakpoint_probability'].mean():.4f}\")\n",
    "        \n",
    "        return test_df\n",
    "\n",
    "print(\"‚úÖ HyenaDNATrainingPipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üöÄ EXECUTION STARTS HERE\n",
    "\n",
    "Now we'll run the complete pipeline step by step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 6: Load and Prepare Data\n",
    "\n",
    "Load the positive and negative CSV files, merge sequences, and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 1: DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize data preparator\n",
    "data_prep = DataPreparator(POSITIVE_CSV, NEGATIVE_CSV)\n",
    "\n",
    "# Load and prepare data\n",
    "train_df = data_prep.load_and_prepare_data()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "display(train_df.head(3))\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 7: Initialize Training Pipeline\n",
    "\n",
    "Create the training pipeline with the specified model and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2: INITIALIZE TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pipeline = HyenaDNATrainingPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Prepare Training and Validation Datasets\n",
    "\n",
    "Split the data into training and validation sets (80/20 split by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 3: PREPARE TRAINING AND VALIDATION DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_dataset, val_dataset = pipeline.prepare_datasets(train_df, val_split=VAL_SPLIT)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(\"\\n‚úÖ Datasets prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 9: Train the Model\n",
    "\n",
    "This is the main training step. It will:\n",
    "- Load the pretrained HyenaDNA model\n",
    "- Add the classification head\n",
    "- Fine-tune on your data\n",
    "- Evaluate on validation set periodically\n",
    "- Save the best model based on F1 score\n",
    "\n",
    "**Expected time:** 30 minutes to several hours depending on:\n",
    "- Dataset size\n",
    "- GPU type (T4 vs A100)\n",
    "- Number of epochs\n",
    "- Model size\n",
    "\n",
    "**Progress indicators:**\n",
    "- Training loss per batch\n",
    "- Validation metrics every 200 steps\n",
    "- Best model automatically saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 4: TRAIN THE MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\nThis may take a while... ‚òï\")\n",
    "print()\n",
    "\n",
    "trainer = pipeline.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 10: View Training Results\n",
    "\n",
    "Display the final training metrics and model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training history\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Extract validation metrics\n",
    "val_metrics = [entry for entry in history if 'eval_accuracy' in entry]\n",
    "\n",
    "if val_metrics:\n",
    "    best_metrics = val_metrics[-1]\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL VALIDATION METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy:  {best_metrics.get('eval_accuracy', 0):.4f}\")\n",
    "    print(f\"Precision: {best_metrics.get('eval_precision', 0):.4f}\")\n",
    "    print(f\"Recall:    {best_metrics.get('eval_recall', 0):.4f}\")\n",
    "    print(f\"F1 Score:  {best_metrics.get('eval_f1', 0):.4f}\")\n",
    "    print(f\"AUC:       {best_metrics.get('eval_auc', 0):.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to: {OUTPUT_DIR}/final_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Step 11: Make Predictions on Test Data\n",
    "\n",
    "Load the test CSV file and make predictions using the trained model.\n",
    "\n",
    "**Output:** CSV file with:\n",
    "- All original columns\n",
    "- `predicted_label` (0 or 1)\n",
    "- `breakpoint_probability` (0.0 to 1.0)\n",
    "- `prediction` (\"Negative\" or \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 5: MAKE PREDICTIONS ON TEST DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predictions_df = pipeline.predict(\n",
    "    test_csv_path=TEST_CSV,\n",
    "    output_path=\"breakpoint_predictions.csv\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Predictions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 12: View Prediction Results\n",
    "\n",
    "Display sample predictions and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTotal test samples: {len(predictions_df)}\")\n",
    "print(f\"Predicted Positive: {(predictions_df['predicted_label'] == 1).sum()} ({(predictions_df['predicted_label'] == 1).sum() / len(predictions_df) * 100:.1f}%)\")\n",
    "print(f\"Predicted Negative: {(predictions_df['predicted_label'] == 0).sum()} ({(predictions_df['predicted_label'] == 0).sum() / len(predictions_df) * 100:.1f}%)\")\n",
    "print(f\"\\nAverage breakpoint probability: {predictions_df['breakpoint_probability'].mean():.4f}\")\n",
    "print(f\"Min probability: {predictions_df['breakpoint_probability'].min():.4f}\")\n",
    "print(f\"Max probability: {predictions_df['breakpoint_probability'].max():.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS (first 5 rows)\")\n",
    "print(\"=\"*70)\n",
    "display(predictions_df[['predicted_label', 'breakpoint_probability', 'prediction']].head())\n",
    "\n",
    "# Probability distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROBABILITY DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(predictions_df['breakpoint_probability'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä (Optional) Step 13: Visualize Results\n",
    "\n",
    "Create visualizations of the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Prediction distribution\n",
    "predictions_df['prediction'].value_counts().plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Probability distribution\n",
    "axes[1].hist(predictions_df['breakpoint_probability'], bins=30, edgecolor='black', alpha=0.7, color='#95E1D3')\n",
    "axes[1].set_title('Breakpoint Probability Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Box plot by prediction\n",
    "predictions_df.boxplot(column='breakpoint_probability', by='prediction', ax=axes[2])\n",
    "axes[2].set_title('Probability by Prediction', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Prediction')\n",
    "axes[2].set_ylabel('Breakpoint Probability')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved as 'prediction_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 14: Save/Download Results\n",
    "\n",
    "Download the results if you're using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Uncomment these lines to download files\n",
    "# from google.colab import files\n",
    "# \n",
    "# # Download predictions\n",
    "# files.download('breakpoint_predictions.csv')\n",
    "# \n",
    "# # Download visualization (if created)\n",
    "# files.download('prediction_analysis.png')\n",
    "# \n",
    "# # Optionally download the model\n",
    "# !zip -r hyenadna_model.zip hyenadna_breakpoint_model/final_model/\n",
    "# files.download('hyenadna_model.zip')\n",
    "\n",
    "print(\"\\nFiles ready for download:\")\n",
    "print(\"  - breakpoint_predictions.csv\")\n",
    "print(\"  - prediction_analysis.png (if visualization was run)\")\n",
    "print(f\"  - {OUTPUT_DIR}/final_model/ (trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ PIPELINE COMPLETE!\n",
    "\n",
    "## What You've Accomplished:\n",
    "\n",
    "1. ‚úÖ Loaded and prepared FusionAI format data\n",
    "2. ‚úÖ Merged 5' and 3' sequences into 20kb inputs\n",
    "3. ‚úÖ Fine-tuned HyenaDNA for binary classification\n",
    "4. ‚úÖ Evaluated model performance on validation data\n",
    "5. ‚úÖ Made predictions on test data\n",
    "6. ‚úÖ Saved results and visualizations\n",
    "\n",
    "## Output Files:\n",
    "\n",
    "- **`breakpoint_predictions.csv`** - Test predictions with probabilities\n",
    "- **`hyenadna_breakpoint_model/final_model/`** - Trained model weights\n",
    "- **`prediction_analysis.png`** - Visualization (if created)\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Analyze results** - Check prediction quality and metrics\n",
    "2. **Adjust hyperparameters** - Try different learning rates, batch sizes, or epochs\n",
    "3. **Use the model** - Apply to new data using the saved model\n",
    "4. **Experiment** - Try different HyenaDNA model sizes for better performance\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ To Use the Trained Model Later:\n",
    "\n",
    "```python\n",
    "# Load the pipeline\n",
    "pipeline = HyenaDNATrainingPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_predictions = pipeline.predict(\n",
    "    test_csv_path=\"new_test_data.csv\",\n",
    "    model_path=\"./hyenadna_breakpoint_model/final_model\",\n",
    "    output_path=\"new_predictions.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the comments in each cell or refer to the README documentation.\n",
    "\n",
    "**Happy predicting! üß¨üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
