{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegmentNT Binary Classifier for DNA Breakpoint Detection\n",
    "\n",
    "This notebook implements a complete pipeline to fine-tune SegmentNT for classifying DNA sequences as positive or negative breakpoints, similar to FusionAI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**What this notebook does:**\n",
    "1. **Prepares data** from FusionAI-format CSV files (positive/negative breakpoints)\n",
    "2. **Merges sequences** (5' + 3' = 20kb input)\n",
    "3. **Fine-tunes SegmentNT** with a binary classification head\n",
    "4. **Makes predictions** on test data\n",
    "\n",
    "**Key Features:**\n",
    "- Uses SegmentNT with Nucleotide Transformer backbone (500M parameters)\n",
    "- Handles sequences up to 30kb (can extend to 50kb)\n",
    "- 6-mer tokenization for efficient DNA encoding\n",
    "- Comprehensive metrics (Accuracy, Precision, Recall, F1, AUC)\n",
    "\n",
    "**About SegmentNT:**\n",
    "- Segmentation model built on Nucleotide Transformer\n",
    "- Trained on 14 different genomic element classes\n",
    "- 562M total parameters (500M backbone + 62M segmentation head)\n",
    "- Single-nucleotide resolution predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation & Setup\n",
    "\n",
    "First, let's install all required dependencies and check our compute environment.\n",
    "\n",
    "**Note:** SegmentNT requires transformers to be installed from source until the next official release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# SegmentNT requires transformers from source\n",
    "# !pip install -q --upgrade git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q torch accelerate pandas scikit-learn\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU Device: NVIDIA A100 80GB PCIe\n",
      "GPU Memory: 84.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akp1/GeneAI/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and setup\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Consider using a GPU runtime (Runtime â†’ Change runtime type â†’ GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Import Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akp1/GeneAI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoConfig\n",
    "from safetensors.torch import load_model, save_model\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration\n",
    "\n",
    "Set all hyperparameters and file paths here. **Update the CSV paths to match your data!**\n",
    "\n",
    "### SegmentNT Model Info:\n",
    "- **Model:** `InstaDeepAI/segment_nt`\n",
    "- **Max sequence length:** 30,000 bp (can extend to 50kb with rescaling)\n",
    "- **Tokenization:** 6-mer tokens (vocabulary size: 4105)\n",
    "- **Parameters:** 562M (500M backbone + 62M segmentation head)\n",
    "- **Architecture:** Nucleotide Transformer + U-Net segmentation head\n",
    "\n",
    "### Important Notes:\n",
    "- The number of DNA tokens (excluding CLS token) must be divisible by 4\n",
    "- For sequences between 30kb-50kb, rescaling_factor needs adjustment\n",
    "- Default training length: 30,000 nucleotides (5001 tokens including CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model: InstaDeepAI/segment_nt\n",
      "Batch size: 4\n",
      "Training epochs: 3\n",
      "Max sequence length: 20000 bp\n"
     ]
    }
   ],
   "source": [
    "# ============ FILE PATHS - UPDATE THESE! ============\n",
    "POSITIVE_CSV = \"datasets/fusion_gene_positive_bp_information_with_class_for_modeling.txt\"  # Path to positive breakpoint CSV\n",
    "NEGATIVE_CSV = \"datasets/fusion_gene_negative_bp_information_with_class_for_modeling.txt\"  # Path to negative breakpoint CSV\n",
    "TEST_CSV = \"datasets/fusion_gene_positive_bp_information_with_class_for_testing.txt\"          # Path to test data CSV\n",
    "COLUMNS = [\"Hgene\",\"Hchr\",\"Hbp\",\"Hstrand\",\"Tgene\",\"Tchr\",\"Tbp\",\"Tstrand\",\"5'-gene sequence (10Kb)\",\"3'-gene sequence (10Kb)\"] # CSV Columns names\n",
    "OUTPUT_DIR = \"./segmentnt_breakpoint_model\" # Where to save the model\n",
    "\n",
    "# ============ MODEL CONFIGURATION ============\n",
    "MODEL_NAME = \"InstaDeepAI/segment_nt\"  # SegmentNT model\n",
    "MAX_LENGTH = 20000  # Maximum sequence length (20kb - within 30kb limit)\n",
    "\n",
    "# Note: MAX_LENGTH must satisfy: (MAX_LENGTH_IN_TOKENS - 1) % 4 == 0\n",
    "# For 20kb sequences, this gives us approximately 3334 tokens\n",
    "# We'll handle this in the tokenizer\n",
    "\n",
    "# ============ TRAINING HYPERPARAMETERS ============\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4          # SegmentNT is larger (562M params), so reduce batch size\n",
    "LEARNING_RATE = 2e-5    # Learning rate\n",
    "WARMUP_STEPS = 500      # Learning rate warmup\n",
    "WEIGHT_DECAY = 0.01     # L2 regularization\n",
    "VAL_SPLIT = 0.2         # Validation split (20%)\n",
    "\n",
    "# ============ DEVICE SETUP ============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH} bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 1: Data Preparation Class\n",
    "\n",
    "This class handles loading and preparing the FusionAI CSV files.\n",
    "\n",
    "**Input CSV Format (tab-separated):**\n",
    "- `Hgene`, `Hchr`, `Hbp`, `Hstrand` - Head gene information\n",
    "- `Tgene`, `Tchr`, `Tbp`, `Tstrand` - Tail gene information\n",
    "- `5'-gene sequence (10Kb)` - 5' sequence (10,000 bp)\n",
    "- `3'-gene sequence (10Kb)` - 3' sequence (10,000 bp)\n",
    "\n",
    "**What it does:**\n",
    "1. Loads positive and negative CSV files\n",
    "2. Merges 5' + 3' sequences â†’ 20kb sequences\n",
    "3. Adds labels (1=positive, 0=negative)\n",
    "4. Combines and shuffles data\n",
    "5. Validates sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakpointDataPreparation:\n",
    "    \"\"\"\n",
    "    Handles loading and preparing breakpoint data from CSV files.\n",
    "    \n",
    "    This class reads FusionAI-format CSV files containing positive and negative\n",
    "    breakpoint examples, merges the 5' and 3' sequences, and prepares the data\n",
    "    for model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize the data preparation class.\n",
    "        \n",
    "        Args:\n",
    "            columns: List of column names for the CSV files\n",
    "        \"\"\"\n",
    "        self.columns = columns\n",
    "        self.seq_5_col = \"5'-gene sequence (10Kb)\"\n",
    "        self.seq_3_col = \"3'-gene sequence (10Kb)\"\n",
    "    \n",
    "    def load_csv(self, filepath: str, label: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a CSV file and add label column.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the CSV file\n",
    "            label: Label to assign (1 for positive, 0 for negative)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with loaded data and label column\n",
    "        \"\"\"\n",
    "        print(f\"Loading {filepath}...\")\n",
    "        df = pd.read_csv(filepath, sep='\\t', names=self.columns)\n",
    "        df['label'] = label\n",
    "        print(f\"  Loaded {len(df)} samples\")\n",
    "        return df\n",
    "    \n",
    "    def merge_sequences(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Merge 5' and 3' sequences into a single combined sequence.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing 5' and 3' sequence columns\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with added 'merged_sequence' column\n",
    "        \"\"\"\n",
    "        print(\"Merging 5' and 3' sequences...\")\n",
    "        df['merged_sequence'] = df[self.seq_5_col] + df[self.seq_3_col]\n",
    "        \n",
    "        # Validate sequences\n",
    "        invalid_chars = df['merged_sequence'].str.contains('[^ACGT]', regex=True, na=False)\n",
    "        if invalid_chars.any():\n",
    "            print(f\"  âš ï¸ Warning: Found {invalid_chars.sum()} sequences with non-ACGT characters\")\n",
    "            print(\"     These will be handled by the tokenizer\")\n",
    "        \n",
    "        # Check sequence lengths\n",
    "        seq_lengths = df['merged_sequence'].str.len()\n",
    "        print(f\"  Sequence length stats:\")\n",
    "        print(f\"    Min: {seq_lengths.min()} bp\")\n",
    "        print(f\"    Max: {seq_lengths.max()} bp\")\n",
    "        print(f\"    Mean: {seq_lengths.mean():.0f} bp\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, positive_csv: str, negative_csv: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and prepare the complete dataset.\n",
    "        \n",
    "        Args:\n",
    "            positive_csv: Path to positive samples CSV\n",
    "            negative_csv: Path to negative samples CSV\n",
    "            \n",
    "        Returns:\n",
    "            Combined and shuffled DataFrame\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"PREPARING TRAINING DATA\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load datasets\n",
    "        df_positive = self.load_csv(positive_csv, label=1)\n",
    "        df_negative = self.load_csv(negative_csv, label=0)\n",
    "        \n",
    "        # Merge sequences\n",
    "        df_positive = self.merge_sequences(df_positive)\n",
    "        df_negative = self.merge_sequences(df_negative)\n",
    "        \n",
    "        # Combine datasets\n",
    "        print(\"\\nCombining datasets...\")\n",
    "        df_combined = pd.concat([df_positive, df_negative], ignore_index=True)\n",
    "        \n",
    "        # Shuffle\n",
    "        df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nâœ… Data preparation complete!\")\n",
    "        print(f\"   Total samples: {len(df_combined)}\")\n",
    "        print(f\"   Positive samples: {(df_combined['label'] == 1).sum()} ({(df_combined['label'] == 1).sum() / len(df_combined) * 100:.1f}%)\")\n",
    "        print(f\"   Negative samples: {(df_combined['label'] == 0).sum()} ({(df_combined['label'] == 0).sum() / len(df_combined) * 100:.1f}%)\")\n",
    "        \n",
    "        return df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¬ Step 2: Custom Dataset Class\n",
    "\n",
    "PyTorch Dataset class for DNA sequences. This handles:\n",
    "- DNA sequence tokenization using SegmentNT's 6-mer tokenizer\n",
    "- Proper padding and truncation\n",
    "- Attention mask generation\n",
    "- Label formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNASequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for DNA sequences.\n",
    "    \n",
    "    Handles tokenization of DNA sequences using SegmentNT's Nucleotide Transformer\n",
    "    tokenizer, which uses 6-mer tokenization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: List[str], labels: List[int], tokenizer, max_length: int):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            sequences: List of DNA sequences\n",
    "            labels: List of labels (0 or 1)\n",
    "            tokenizer: SegmentNT tokenizer\n",
    "            max_length: Maximum token length (not base pairs)\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Calculate token length that satisfies (token_length - 1) % 4 == 0\n",
    "        # SegmentNT requires this for its downsampling blocks\n",
    "        # For 20kb sequences (~3333 tokens), we use 3333 tokens\n",
    "        base_tokens = int(max_length / 6)  # Approximate tokens for 6-mer tokenization\n",
    "        # Find nearest valid token length\n",
    "        while (base_tokens - 1) % 4 != 0:\n",
    "            base_tokens += 1\n",
    "        \n",
    "        self.max_token_length = base_tokens\n",
    "        print(f\"Using max token length: {self.max_token_length} (for ~{max_length} bp sequences)\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single item from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the item\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing input_ids, attention_mask, and labels\n",
    "        \"\"\"\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize sequence\n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 3: SegmentNT Classification Model\n",
    "\n",
    "Wraps the SegmentNT model with a binary classification head.\n",
    "\n",
    "**Architecture:**\n",
    "1. **SegmentNT Encoder** (500M params) - Nucleotide Transformer backbone\n",
    "2. **U-Net Segmentation Head** (62M params) - Original SegmentNT head\n",
    "3. **Classification Head** - Our custom binary classifier\n",
    "   - Global average pooling over sequence\n",
    "   - Dropout (0.1)\n",
    "   - Linear layer â†’ 2 classes\n",
    "\n",
    "**Note:** We'll use the hidden states from SegmentNT and add our own classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentNTForBinaryClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    SegmentNT model with a binary classification head.\n",
    "    \n",
    "    Takes the SegmentNT encoder and adds a classification head on top\n",
    "    for binary classification of DNA sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, num_labels: int = 2, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the classification model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the SegmentNT model\n",
    "            num_labels: Number of output classes (2 for binary classification)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"Loading SegmentNT model: {model_name}...\")\n",
    "        # config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        # print(f\"Intermediate size: {getattr(config, 'intermediate_size', 'NOT SET')}\")\n",
    "\n",
    "        self.segmentnt = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            ignore_mismatched_sizes=True  # We want to catch mismatches\n",
    "        )\n",
    "        \n",
    "        # Get hidden size from the model config\n",
    "        self.hidden_size = self.segmentnt.hidden_size\n",
    "        print(f\"  Hidden size: {self.hidden_size}\")\n",
    "        print(f\"  Model loaded successfully!\")\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_labels)\n",
    "        \n",
    "        print(f\"  Added classification head: {self.hidden_size} -> {num_labels}\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tokenized input sequences\n",
    "            attention_mask: Attention mask for padded tokens\n",
    "            labels: Ground truth labels (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing loss (if labels provided) and logits\n",
    "        \"\"\"\n",
    "        # Get SegmentNT outputs\n",
    "        outputs = self.segmentnt(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Global average pooling over the sequence dimension\n",
    "        # Only pool over non-padded tokens using attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask to match hidden states shape\n",
    "            mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "            sum_hidden = torch.sum(hidden_states * mask, dim=1)\n",
    "            sum_mask = torch.sum(mask, dim=1)\n",
    "            pooled_output = sum_hidden / sum_mask\n",
    "        else:\n",
    "            pooled_output = torch.mean(hidden_states, dim=1)\n",
    "        \n",
    "        # Apply dropout and classifier\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 4: Evaluation Metrics\n",
    "\n",
    "Function to compute comprehensive classification metrics.\n",
    "\n",
    "**Metrics computed:**\n",
    "- **Accuracy** - Overall correctness\n",
    "- **Precision** - Of predicted positives, how many are correct?\n",
    "- **Recall** - Of actual positives, how many did we find?\n",
    "- **F1 Score** - Harmonic mean of precision and recall\n",
    "- **AUC-ROC** - Area under the ROC curve (model discrimination ability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple of (predictions, labels)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Convert logits to predictions\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()[:, 1]\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 5: Complete Training Pipeline\n",
    "\n",
    "This class orchestrates the entire workflow:\n",
    "1. Data preparation\n",
    "2. Model initialization\n",
    "3. Training with Hugging Face Trainer\n",
    "4. Evaluation\n",
    "5. Model saving\n",
    "6. Predictions\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic train/validation split\n",
    "- FP16 mixed precision training\n",
    "- Gradient accumulation\n",
    "- Learning rate scheduling with warmup\n",
    "- Checkpoint saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentNTTrainingPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for training SegmentNT on breakpoint classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, output_dir: str, max_length: int):\n",
    "        \"\"\"\n",
    "        Initialize the training pipeline.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the SegmentNT model\n",
    "            output_dir: Directory to save model and outputs\n",
    "            max_length: Maximum sequence length in base pairs\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load tokenizer\n",
    "        print(f\"Loading SegmentNT tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"  Tokenizer loaded!\")\n",
    "        print(f\"  Vocabulary size: {len(self.tokenizer)}\")\n",
    "        print(f\"  Tokenization: 6-mer based\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def prepare_datasets(self, df: pd.DataFrame, val_split: float = 0.2):\n",
    "        \"\"\"\n",
    "        Prepare training and validation datasets.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with 'merged_sequence' and 'label' columns\n",
    "            val_split: Fraction of data to use for validation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_dataset, val_dataset)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PREPARING DATASETS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Split data\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, test_size=val_split, random_state=42, stratify=df['label']\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(train_df)}\")\n",
    "        print(f\"Validation samples: {len(val_df)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DNASequenceDataset(\n",
    "            sequences=train_df['merged_sequence'].tolist(),\n",
    "            labels=train_df['label'].tolist(),\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = DNASequenceDataset(\n",
    "            sequences=val_df['merged_sequence'].tolist(),\n",
    "            labels=val_df['label'].tolist(),\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… Datasets prepared!\")\n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, num_epochs: int, \n",
    "              batch_size: int, learning_rate: float, warmup_steps: int,\n",
    "              weight_decay: float):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \n",
    "        Args:\n",
    "            train_dataset: Training dataset\n",
    "            val_dataset: Validation dataset\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size\n",
    "            learning_rate: Learning rate\n",
    "            warmup_steps: Number of warmup steps\n",
    "            weight_decay: L2 regularization weight\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INITIALIZING MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = SegmentNTForBinaryClassification(self.model_name)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TRAINING CONFIGURATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Epochs: {num_epochs}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Learning rate: {learning_rate}\")\n",
    "        print(f\"Warmup steps: {warmup_steps}\")\n",
    "        print(f\"Weight decay: {weight_decay}\")\n",
    "        use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "             output_dir=self.output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=weight_decay,        \n",
    "            max_grad_norm=1.0,             \n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            bf16=use_bf16,\n",
    "            fp16=False, \n",
    "            logging_dir=f'{self.output_dir}/logs',\n",
    "            logging_steps=50,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            dataloader_num_workers=4,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\",\n",
    "            save_safetensors=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STARTING TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Train\n",
    "        self.trainer.train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TRAINING COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on validation data.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EVALUATING MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        metrics = self.trainer.evaluate()\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print(f\"  Accuracy:  {metrics['eval_accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['eval_precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['eval_recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {metrics['eval_f1']:.4f}\")\n",
    "        print(f\"  AUC-ROC:   {metrics['eval_auc']:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save_model(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Save the trained model.\n",
    "        \n",
    "        Args:\n",
    "            save_path: Path to save the model (default: output_dir/final_model)\n",
    "        \"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = os.path.join(self.output_dir, 'final_model')\n",
    "        \n",
    "        print(f\"\\nSaving model to {save_path}...\")\n",
    "        save_model(self.model, f\"{self.output_dir}/final_model/model.safetensors\")\n",
    "        self.tokenizer.save_pretrained(f\"{self.output_dir}/final_model\")\n",
    "        print(\"âœ… Model saved!\")\n",
    "    \n",
    "    def predict(self, test_csv_path: str, model_path: str = None, \n",
    "                output_path: str = \"predictions.csv\", batch_size: int = 8) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Make predictions on test data.\n",
    "        \n",
    "        Args:\n",
    "            test_csv_path: Path to test CSV file\n",
    "            model_path: Path to trained model (default: use current model)\n",
    "            output_path: Path to save predictions CSV\n",
    "            batch_size: Batch size for inference\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with predictions\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MAKING PREDICTIONS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_csv_path}...\")\n",
    "        df_test = pd.read_csv(test_csv_path, sep='\\t', names=COLUMNS)\n",
    "        df_test['merged_sequence'] = df_test[\"5'-gene sequence (10Kb)\"] + df_test[\"3'-gene sequence (10Kb)\"]\n",
    "        print(f\"  Loaded {len(df_test)} test samples\")\n",
    "        \n",
    "        # Load model if path provided\n",
    "        if self.model == None:\n",
    "            print(f\"\\nLoading model from {model_path}...\")\n",
    "            self.model = HyenaDNAClassifier(self.model_name, num_labels=2)\n",
    "            load_model(self.model, f\"{model_path}/model.safetensors\")\n",
    "            self.model.to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(f\"{OUTPUT_DIR}/final_model\",\n",
    "            trust_remote_code=True)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_dataset = DNASequenceDataset(\n",
    "            sequences=df_test['merged_sequence'].tolist(),\n",
    "            labels=[0] * len(df_test),  # Dummy labels\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                # Get predictions and probabilities\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of positive class\n",
    "        \n",
    "        # Add predictions to dataframe\n",
    "        df_test['predicted_label'] = all_predictions\n",
    "        df_test['breakpoint_probability'] = all_probabilities\n",
    "        df_test['prediction'] = df_test['predicted_label'].map({0: 'Negative', 1: 'Positive'})\n",
    "        \n",
    "        # Save predictions\n",
    "        print(f\"\\nSaving predictions to {output_path}...\")\n",
    "        df_test.to_csv(output_path, index=False, sep='\\t')\n",
    "        print(\"âœ… Predictions saved!\")\n",
    "        \n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 6: Prepare Data\n",
    "\n",
    "Load and prepare the training data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING TRAINING DATA\n",
      "======================================================================\n",
      "Loading datasets/fusion_gene_positive_bp_information_with_class_for_modeling.txt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 30745 samples\n",
      "Loading datasets/fusion_gene_negative_bp_information_with_class_for_modeling.txt...\n",
      "  Loaded 20000 samples\n",
      "Merging 5' and 3' sequences...\n",
      "  âš ï¸ Warning: Found 15 sequences with non-ACGT characters\n",
      "     These will be handled by the tokenizer\n",
      "  Sequence length stats:\n",
      "    Min: 20000 bp\n",
      "    Max: 20000 bp\n",
      "    Mean: 20000 bp\n",
      "Merging 5' and 3' sequences...\n",
      "  âš ï¸ Warning: Found 116 sequences with non-ACGT characters\n",
      "     These will be handled by the tokenizer\n",
      "  Sequence length stats:\n",
      "    Min: 20000 bp\n",
      "    Max: 20000 bp\n",
      "    Mean: 20000 bp\n",
      "\n",
      "Combining datasets...\n",
      "\n",
      "âœ… Data preparation complete!\n",
      "   Total samples: 50745\n",
      "   Positive samples: 30745 (60.6%)\n",
      "   Negative samples: 20000 (39.4%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize data preparation\n",
    "data_prep = BreakpointDataPreparation(columns=COLUMNS)\n",
    "\n",
    "# Prepare data\n",
    "df_combined = data_prep.prepare_data(\n",
    "    positive_csv=POSITIVE_CSV,\n",
    "    negative_csv=NEGATIVE_CSV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Step 7: Initialize Pipeline and Prepare Datasets\n",
    "\n",
    "Create the training pipeline and prepare train/validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SegmentNT tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizer loaded!\n",
      "  Vocabulary size: 4107\n",
      "  Tokenization: 6-mer based\n",
      "\n",
      "======================================================================\n",
      "PREPARING DATASETS\n",
      "======================================================================\n",
      "Training samples: 40596\n",
      "Validation samples: 10149\n",
      "Using max token length: 3333 (for ~20000 bp sequences)\n",
      "Using max token length: 3333 (for ~20000 bp sequences)\n",
      "\n",
      "âœ… Datasets prepared!\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = SegmentNTTrainingPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, val_dataset = pipeline.prepare_datasets(\n",
    "    df_combined, \n",
    "    val_split=VAL_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš‚ Step 8: Train the Model\n",
    "\n",
    "Train SegmentNT on the breakpoint classification task.\n",
    "\n",
    "**This will take some time depending on:**\n",
    "- GPU type (A100 recommended)\n",
    "- Dataset size\n",
    "- Number of epochs\n",
    "\n",
    "**Progress indicators:**\n",
    "- Training loss (should decrease)\n",
    "- Evaluation metrics per epoch\n",
    "- Checkpoint saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INITIALIZING MODEL\n",
      "======================================================================\n",
      "Loading SegmentNT model: InstaDeepAI/segment_nt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/InstaDeepAI/segment_nt:\n",
      "- segment_nt_config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/InstaDeepAI/segment_nt:\n",
      "- modeling_segment_nt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'find_pruneable_heads_and_indices' from 'transformers.modeling_utils' (/home/akp1/GeneAI/venv/lib/python3.12/site-packages/transformers/modeling_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mSegmentNTTrainingPipeline.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, num_epochs, batch_size, learning_rate, warmup_steps, weight_decay)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSegmentNTForBinaryClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.model.to(device)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mSegmentNTForBinaryClassification.__init__\u001b[39m\u001b[34m(self, model_name, num_labels, dropout)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading SegmentNT model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# print(f\"Intermediate size: {getattr(config, 'intermediate_size', 'NOT SET')}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.segmentnt = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We want to catch mismatches\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Get hidden size from the model config\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.hidden_size = \u001b[38;5;28mself\u001b[39m.segmentnt.hidden_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GeneAI/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:354\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = adapter_kwargs\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    358\u001b[39m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[32m    359\u001b[39m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[32m    360\u001b[39m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GeneAI/venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:584\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    573\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    574\u001b[39m     repo_id,\n\u001b[32m    575\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m     repo_type=repo_type,\n\u001b[32m    583\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GeneAI/venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:310\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/InstaDeepAI/segment_nt/cc91ff81aa5f0b7d0dce23b1b3bb1e5f8b9184d3/modeling_segment_nt.py:37\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     27\u001b[39m     add_start_docstrings,\n\u001b[32m     28\u001b[39m     add_start_docstrings_to_model_forward,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[32m     32\u001b[39m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     TokenClassifierOutput,\n\u001b[32m     36\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     PreTrainedModel,\n\u001b[32m     39\u001b[39m     find_pruneable_heads_and_indices,\n\u001b[32m     40\u001b[39m     prune_linear_layer,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msegment_nt_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SegmentNTConfig\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'find_pruneable_heads_and_indices' from 'transformers.modeling_utils' (/home/akp1/GeneAI/venv/lib/python3.12/site-packages/transformers/modeling_utils.py)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pipeline.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 9: Evaluate Model Performance\n",
    "\n",
    "Evaluate the trained model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_metrics = pipeline.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"   Accuracy:  {eval_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"   Precision: {eval_metrics['eval_precision']:.4f}\")\n",
    "print(f\"   Recall:    {eval_metrics['eval_recall']:.4f}\")\n",
    "print(f\"   F1 Score:  {eval_metrics['eval_f1']:.4f}\")\n",
    "print(f\"   AUC-ROC:   {eval_metrics['eval_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 10: Save the Trained Model\n",
    "\n",
    "Save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pipeline.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 11: Make Predictions on Test Data\n",
    "\n",
    "Load the test CSV file and make predictions using the trained model.\n",
    "\n",
    "**Output:** CSV file with:\n",
    "- All original columns\n",
    "- `predicted_label` (0 or 1)\n",
    "- `breakpoint_probability` (0.0 to 1.0)\n",
    "- `prediction` (\"Negative\" or \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 5: MAKE PREDICTIONS ON TEST DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predictions_df = pipeline.predict(\n",
    "    test_csv_path=TEST_CSV,\n",
    "    output_path=\"breakpoint_predictions.csv\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Predictions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Step 12: View Prediction Results\n",
    "\n",
    "Display sample predictions and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTotal test samples: {len(predictions_df)}\")\n",
    "print(f\"Predicted Positive: {(predictions_df['predicted_label'] == 1).sum()} ({(predictions_df['predicted_label'] == 1).sum() / len(predictions_df) * 100:.1f}%)\")\n",
    "print(f\"Predicted Negative: {(predictions_df['predicted_label'] == 0).sum()} ({(predictions_df['predicted_label'] == 0).sum() / len(predictions_df) * 100:.1f}%)\")\n",
    "print(f\"\\nAverage breakpoint probability: {predictions_df['breakpoint_probability'].mean():.4f}\")\n",
    "print(f\"Min probability: {predictions_df['breakpoint_probability'].min():.4f}\")\n",
    "print(f\"Max probability: {predictions_df['breakpoint_probability'].max():.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS (first 5 rows)\")\n",
    "print(\"=\"*70)\n",
    "display(predictions_df[['predicted_label', 'breakpoint_probability', 'prediction']].head())\n",
    "\n",
    "# Probability distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROBABILITY DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(predictions_df['breakpoint_probability'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š (Optional) Step 13: Visualize Results\n",
    "\n",
    "Create visualizations of the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Prediction distribution\n",
    "predictions_df['prediction'].value_counts().plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Probability distribution\n",
    "axes[1].hist(predictions_df['breakpoint_probability'], bins=30, edgecolor='black', alpha=0.7, color='#95E1D3')\n",
    "axes[1].set_title('Breakpoint Probability Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Box plot by prediction\n",
    "predictions_df.boxplot(column='breakpoint_probability', by='prediction', ax=axes[2])\n",
    "axes[2].set_title('Probability by Prediction', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Prediction')\n",
    "axes[2].set_ylabel('Breakpoint Probability')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualization saved as 'prediction_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 14: Save/Download Results\n",
    "\n",
    "Download the results if you're using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Uncomment these lines to download files\n",
    "# from google.colab import files\n",
    "# \n",
    "# # Download predictions\n",
    "# files.download('breakpoint_predictions.csv')\n",
    "# \n",
    "# # Download visualization (if created)\n",
    "# files.download('prediction_analysis.png')\n",
    "# \n",
    "# # Optionally download the model\n",
    "# !zip -r segmentnt_model.zip segmentnt_breakpoint_model/final_model/\n",
    "# files.download('segmentnt_model.zip')\n",
    "\n",
    "print(\"\\nFiles ready for download:\")\n",
    "print(\"  - breakpoint_predictions.csv\")\n",
    "print(\"  - prediction_analysis.png (if visualization was run)\")\n",
    "print(f\"  - {OUTPUT_DIR}/final_model/ (trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# âœ… PIPELINE COMPLETE!\n",
    "\n",
    "## What You've Accomplished:\n",
    "\n",
    "1. âœ… Loaded and prepared FusionAI format data\n",
    "2. âœ… Merged 5' and 3' sequences into 20kb inputs\n",
    "3. âœ… Fine-tuned SegmentNT for binary classification\n",
    "4. âœ… Evaluated model performance on validation data\n",
    "5. âœ… Made predictions on test data\n",
    "6. âœ… Saved results and visualizations\n",
    "\n",
    "## Output Files:\n",
    "\n",
    "- **`breakpoint_predictions.csv`** - Test predictions with probabilities\n",
    "- **`segmentnt_breakpoint_model/final_model/`** - Trained model weights\n",
    "- **`prediction_analysis.png`** - Visualization (if created)\n",
    "\n",
    "## About SegmentNT:\n",
    "\n",
    "SegmentNT is a powerful DNA foundation model that:\n",
    "- Uses Nucleotide Transformer (500M parameters) as backbone\n",
    "- Employs U-Net segmentation architecture\n",
    "- Trained on 14 different genomic element classes\n",
    "- Provides single-nucleotide resolution predictions\n",
    "- Can handle sequences up to 50kb (with rescaling)\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Analyze results** - Check prediction quality and metrics\n",
    "2. **Adjust hyperparameters** - Try different learning rates, batch sizes, or epochs\n",
    "3. **Use the model** - Apply to new data using the saved model\n",
    "4. **Experiment with sequence length** - Try longer sequences (up to 30kb or 50kb with rescaling)\n",
    "5. **Compare with HyenaDNA** - Evaluate performance differences\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ To Use the Trained Model Later:\n",
    "\n",
    "```python\n",
    "# Load the pipeline\n",
    "pipeline = SegmentNTTrainingPipeline(\n",
    "    model_name=MODEL_NAME,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_predictions = pipeline.predict(\n",
    "    test_csv_path=\"new_test_data.csv\",\n",
    "    model_path=\"./segmentnt_breakpoint_model/final_model\",\n",
    "    output_path=\"new_predictions.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š References:\n",
    "\n",
    "- **SegmentNT Paper:** [Segmenting the genome at single-nucleotide resolution with DNA foundation models](https://www.biorxiv.org/content/biorxiv/early/2024/03/15/2024.03.14.584712.full.pdf)\n",
    "- **Model Hub:** [InstaDeepAI/segment_nt](https://huggingface.co/InstaDeepAI/segment_nt)\n",
    "- **GitHub:** [Nucleotide Transformer Repository](https://github.com/instadeepai/nucleotide-transformer)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the comments in each cell or refer to the SegmentNT documentation.\n",
    "\n",
    "**Happy predicting! ðŸ§¬ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
